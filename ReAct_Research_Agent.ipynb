{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "Yj1UVzpgtofI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries which are being installed are:\n",
        "1. `arxiv:` This library provides a way to interact with the `arXiv API`, allowing you to *search for and retrieve academic papers* from the arXiv preprint server.\n",
        "\n",
        "2. `llama_index:` This library (formerly known as `GPT Index`) helps in building an *index over your data*, allowing for *efficient querying* using natural language. It can be **used with large language models (LLMs) for question answering and other tasks**.\n",
        "\n",
        "3. `llama-index-llms-mistralai:` This is an `extension to llama_index` that enables *integration with Mistral AI*, a **LLM provider**. It enables you to use Mistral's LLMs with llama_index.\n",
        "\n",
        "4. `llama-index-embeddings-mistralai:` Similar to the previous one, this extension *allows you to use Mistral's embedding models with llama_index.*\n",
        "\n",
        "**NOTE -** Embeddings are numerical representations of text that are used for similarity search within the index.\n",
        "\n",
        "In short, `this line of code is setting up the necessary libraries to work with arXiv data, build an index, and leverage Mistral's LLMs and embedding models for querying and processing information.`"
      ],
      "metadata": {
        "id": "vZ3u5JaftsAt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmQfZJbBZrnj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ae97d88-79d3-4292-8b7f-9d648a0b9b48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting llama_index\n",
            "  Downloading llama_index-0.12.23-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting llama-index-llms-mistralai\n",
            "  Downloading llama_index_llms_mistralai-0.4.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting llama-index-embeddings-mistralai\n",
            "  Downloading llama_index_embeddings_mistralai-0.3.0-py3-none-any.whl.metadata (696 bytes)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/dist-packages (from arxiv) (2.32.3)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl.metadata (727 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.1 (from llama_index)\n",
            "  Downloading llama_index_cli-0.4.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.23 (from llama_index)\n",
            "  Downloading llama_index_core-0.12.23.post2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama_index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.8-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama_index)\n",
            "  Downloading llama_index_llms_openai-0.3.25-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama_index)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama_index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_readers_file-0.4.6-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama_index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama_index) (3.9.1)\n",
            "Collecting mistralai>=1.0.0 (from llama-index-llms-mistralai)\n",
            "  Downloading mistralai-1.5.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama_index) (1.61.1)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.23->llama_index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.23->llama_index) (2.0.38)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.23->llama_index) (3.11.13)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.23->llama_index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.23->llama_index) (1.2.18)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.23->llama_index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.23->llama_index)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.23->llama_index) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.23->llama_index) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.23->llama_index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.23->llama_index) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.23->llama_index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.23->llama_index) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.23->llama_index) (2.10.6)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.23->llama_index) (9.0.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.23->llama_index)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.23->llama_index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.23->llama_index) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.23->llama_index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.23->llama_index) (1.17.2)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.13 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud-0.1.14-py3-none-any.whl.metadata (902 bytes)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (4.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2.2.2)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index)\n",
            "  Downloading pypdf-5.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama_index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_parse-0.6.4.post1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting eval-type-backport>=0.2.0 (from mistralai>=1.0.0->llama-index-llms-mistralai)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting jsonpath-python>=1.0.6 (from mistralai>=1.0.0->llama-index-llms-mistralai)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from mistralai>=1.0.0->llama-index-llms-mistralai) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama_index) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv) (2025.1.31)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.23->llama_index) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.23->llama_index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.23->llama_index) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.23->llama_index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.23->llama_index) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.23->llama_index) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.23->llama_index) (1.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.23->llama_index) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.23->llama_index) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.23->llama_index) (0.14.0)\n",
            "Collecting llama-cloud-services>=0.6.4 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading llama_cloud_services-0.6.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama_index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama_index) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama_index) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.23->llama_index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.23->llama_index) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->mistralai>=1.0.0->llama-index-llms-mistralai) (1.17.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.23->llama_index) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.23->llama_index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.23->llama_index)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama_index) (2025.1)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.4->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama_index)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.23->llama_index) (24.2)\n",
            "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Downloading llama_index-0.12.23-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_llms_mistralai-0.4.0-py3-none-any.whl (8.0 kB)\n",
            "Downloading llama_index_embeddings_mistralai-0.3.0-py3-none-any.whl (2.6 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_agent_openai-0.4.6-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.1-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_core-0.12.23.post2-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.8-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_llms_openai-0.3.25-py3-none-any.whl (16 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.6-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading mistralai-1.5.1-py3-none-any.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Downloading llama_cloud-0.1.14-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.7/261.7 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.6.4.post1-py3-none-any.whl (4.9 kB)\n",
            "Downloading pypdf-5.3.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading llama_cloud_services-0.6.5-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=7cbc65e21f388c94c98d33e9bd5892e628274ac795ed7cb982bb377e051abe04\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: striprtf, sgmllib3k, filetype, dirtyjson, python-dotenv, pypdf, mypy-extensions, marshmallow, jsonpath-python, feedparser, eval-type-backport, typing-inspect, tiktoken, arxiv, mistralai, llama-cloud, dataclasses-json, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-llms-mistralai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-embeddings-mistralai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama_index\n",
            "Successfully installed arxiv-2.1.3 dataclasses-json-0.6.7 dirtyjson-1.0.8 eval-type-backport-0.2.2 feedparser-6.0.11 filetype-1.2.0 jsonpath-python-1.0.6 llama-cloud-0.1.14 llama-cloud-services-0.6.5 llama-index-agent-openai-0.4.6 llama-index-cli-0.4.1 llama-index-core-0.12.23.post2 llama-index-embeddings-mistralai-0.3.0 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.8 llama-index-llms-mistralai-0.4.0 llama-index-llms-openai-0.3.25 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.6 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.4.post1 llama_index-0.12.23 marshmallow-3.26.1 mistralai-1.5.1 mypy-extensions-1.0.0 pypdf-5.3.1 python-dotenv-1.0.1 sgmllib3k-1.0.0 striprtf-0.0.26 tiktoken-0.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install arxiv llama_index llama-index-llms-mistralai llama-index-embeddings-mistralai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ],
      "metadata": {
        "id": "jOlTSHfXvlJg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDiqtxq2Zrnk"
      },
      "outputs": [],
      "source": [
        "# Importing libraries for HTTP requests and ArXiv interaction\n",
        "import requests  # For making HTTP requests to download files and interact with APIs\n",
        "import arxiv  # For interacting with the ArXiv preprint server to search and retrieve papers\n",
        "\n",
        "# Importing components from the LlamaIndex library\n",
        "from llama_index.llms.mistralai import MistralAI as MistralLLM  # Importing Mistral's LLM and renaming it for convenience\n",
        "from llama_index.embeddings.mistralai import MistralAIEmbedding as MistralEmb  # Importing Mistral's embedding model and renaming it\n",
        "from llama_index.core import (  # Importing core components for indexing and querying\n",
        "    VectorStoreIndex,  # Used to create and manage the index of documents\n",
        "    Document,  # Represents a single document in the index\n",
        "    StorageContext,  # Handles saving and loading the index to/from disk\n",
        "    load_index_from_storage,  # Function to load a previously saved index\n",
        "    Settings,  # Allows for configuring various settings of the LlamaIndex library\n",
        ")\n",
        "from llama_index.core.tools import (  # Importing tools for building agents\n",
        "    FunctionTool,  # Allows wrapping Python functions as tools for the agent\n",
        "    QueryEngineTool,  # Wraps a query engine as a tool for the agent\n",
        ")\n",
        "from llama_index.core.agent import ReActAgent  # Importing the ReAct agent class"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtaining API Token & Instantiating the models"
      ],
      "metadata": {
        "id": "Fd-kie9WwjJf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiHWlPNQZrnl"
      },
      "outputs": [],
      "source": [
        "# Securely obtain the API token\n",
        "api_token= \"pSrD5RTTXSVa0YwP8px2IKXOHf08KB5X\"\n",
        "\n",
        "# Instantiate the language model and the embedding model\n",
        "model_instance = MistralLLM(api_key=api_token, model='mistral-large-latest')\n",
        "embed_instance = MistralEmb(model_name=\"mistral-embed\", api_key=api_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JtM8NkrZrnl"
      },
      "source": [
        "## Step 1 – Define an ArXiv Query Function\n",
        "\n",
        "* A function is defined to query ArXiv based on a given topic.\n",
        "* It constructs a search query and uses the ArXiv API to return details (title, authors, abstract, links, etc.) for a specified number of recent papers.\n",
        "* This function provides the dynamic “fetch” capability in case the topic isn’t already in the local knowledge base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48PI_2qsZrnm"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict, Any  # Import necessary type hints\n",
        "'''\n",
        "    Queries ArXiv for papers based on the provided topic.\n",
        "    Returns a list of dictionaries with paper details.\n",
        "'''\n",
        "def query_arxiv_papers(topic_str: str, max_results: int) -> List[Dict[str, Any]]:\n",
        "\n",
        "    # Construct the search query string\n",
        "    search_term = f'all:\"{topic_str}\"'\n",
        "\n",
        "    # Create an ArXiv Search object with search parameters\n",
        "    search_query = arxiv.Search(\n",
        "        query=search_term,  # The search query\n",
        "        max_results=max_results,  # Maximum number of results to fetch\n",
        "        sort_by=arxiv.SortCriterion.SubmittedDate,  # Sort by submission date\n",
        "        sort_order=arxiv.SortOrder.Descending  # Sort in descending order (newest first)\n",
        "    )\n",
        "\n",
        "    papers_list = []  # Initialize an empty list to store paper details\n",
        "    try:\n",
        "        client = arxiv.Client()  # Create an ArXiv API client\n",
        "        # Iterate through the search results\n",
        "        for result in client.results(search_query):\n",
        "            # Extract paper information and store it in a dictionary\n",
        "            paper_info = {\n",
        "                'paper_title': result.title,\n",
        "                'authors_list': [author.name for author in result.authors] if result.authors else [],\n",
        "                'abstract_text': result.summary,\n",
        "                'date_published': result.published,\n",
        "                'journal_details': result.journal_ref,\n",
        "                'doi_number': result.doi,\n",
        "                'primary_category': result.primary_category,\n",
        "                'all_categories': result.categories,\n",
        "                'pdf_link': result.pdf_url,\n",
        "                'arxiv_link': result.entry_id\n",
        "            }\n",
        "            papers_list.append(paper_info)  # Add the paper info to the list\n",
        "    except Exception as error:\n",
        "        # Handle any errors during the ArXiv query\n",
        "        # In production, use logging instead of print.\n",
        "        print(f\"Error querying ArXiv for '{topic_str}': {error}\")\n",
        "\n",
        "    return papers_list  # Return the list of paper details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CVEHPJSZrnm"
      },
      "source": [
        "## Step 2 – Retrieve Initial Papers from ArXiv\n",
        "\n",
        "* By calling the ArXiv query function, you fetch a batch of papers (e.g., on \"Language Models\") to serve as your initial knowledge base.\n",
        "\n",
        "* This set will later be converted into documents and indexed for quick retrieval.\n",
        "\n",
        "* This function (`query_arxiv_papers`) directly queries ArXiv to retrieve the latest paper details if they aren’t found in the index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRvyqWnhZrnm"
      },
      "outputs": [],
      "source": [
        "# Language Models is a search term while 2 is the number of papers\n",
        "papers_data = query_arxiv_papers(\"Language Models\", 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KopKtZYZrnm"
      },
      "source": [
        "## Step 3 – Create Document Objects from Paper Metadata\n",
        "\n",
        "* Each paper’s metadata is formatted into a single text block.\n",
        "* These text blocks are then converted into Document objects that will be processed by the embedding model.\n",
        "* The purpose here is to prepare the raw paper data for vector indexing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKqIBkUdZrnm"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict, Any  # Import necessary type hints\n",
        "from llama_index.core import Document  # Import the Document class from LlamaIndex\n",
        "\n",
        "# Converts a list of paper metadata dictionaries into a list of Document objects.\n",
        "def create_documents(papers_info: List[Dict[str, Any]]) -> List[Document]:\n",
        "    documents = []  # Initialize an empty list to store Document objects\n",
        "    for paper in papers_info:  # Iterate through each paper dictionary in the input list\n",
        "        try:\n",
        "            # Format the paper metadata into a single string\n",
        "            content = (\n",
        "                f\"Title: {paper.get('paper_title', 'N/A')}\\n\"\n",
        "                f\"Authors: {', '.join(paper.get('authors_list', []))}\\n\"\n",
        "                f\"Abstract: {paper.get('abstract_text', 'N/A')}\\n\"\n",
        "                f\"Published: {paper.get('date_published', 'N/A')}\\n\"\n",
        "                f\"Journal: {paper.get('journal_details', 'N/A')}\\n\"\n",
        "                f\"DOI: {paper.get('doi_number', 'N/A')}\\n\"\n",
        "                f\"Primary Category: {paper.get('primary_category', 'N/A')}\\n\"\n",
        "                f\"Categories: {', '.join(paper.get('all_categories', []))}\\n\"\n",
        "                f\"PDF Link: {paper.get('pdf_link', 'N/A')}\\n\"\n",
        "                f\"Arxiv Link: {paper.get('arxiv_link', 'N/A')}\\n\"\n",
        "            )\n",
        "            # Create a Document object with the formatted content and add it to the list\n",
        "            documents.append(Document(text=content))\n",
        "        except Exception as error:\n",
        "            # Handle any errors during document creation\n",
        "            # In production, replace print with logging.error(...)\n",
        "            print(f\"Error processing paper '{paper.get('paper_title', 'Unknown')}': {error}\")\n",
        "    return documents  # Return the list of Document objects\n",
        "\n",
        "# Assuming 'papers_data' is a list of paper metadata dictionaries obtained from a previous step\n",
        "doc_objects = create_documents(papers_data)  # Call the function to create Document objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQaMG22aZrnn"
      },
      "source": [
        "## Step 4 – Build and Persist the Vector Index\n",
        "\n",
        "* Using the embedding model, we convert the documents into vector representations and build a vector index.\n",
        "\n",
        "* Settings like 'chunk size and overlap' controls how the text is segmented for better embedding.\n",
        "\n",
        "* Persisting the index to disk means you don’t have to re-run this expensive step every time you launch the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXMnZfgAZrnn"
      },
      "outputs": [],
      "source": [
        "Settings.chunk_size = 1024\n",
        "Settings.chunk_overlap = 50\n",
        "\n",
        "# Build the index from the documents using the embedding model\n",
        "doc_index = VectorStoreIndex.from_documents(doc_objects, embed_model=embed_instance)\n",
        "\n",
        "# Persist the index to avoid re-indexing on every run\n",
        "doc_index.storage_context.persist('doc_index/')\n",
        "\n",
        "# Reload the index from storage\n",
        "storage_ctx = StorageContext.from_defaults(persist_dir='doc_index/')\n",
        "doc_index = load_index_from_storage(storage_ctx, embed_model=embed_instance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uM3LgzzZrnn"
      },
      "source": [
        "#### Several important steps are happening:\n",
        "\n",
        "1. **Chunking the Documents**  \n",
        "   - `Settings.chunk_size = 1024` and `Settings.chunk_overlap = 50` determine how to split each document into smaller, more manageable segments (“chunks”).  \n",
        "   - \\[**Why chunk?**\\] Large text blocks can be harder to embed semantically in a single shot, and also hamper fine-grained search. By slicing text into 1024-token pieces with some overlap between consecutive chunks, you:\n",
        "     - Avoid exceeding token or model limits.  \n",
        "     - Preserve continuity, because a 50-token overlap means each chunk still has some context from the end of the previous chunk. This helps keep meaning consistent at chunk boundaries.\n",
        "\n",
        "2. **Creating a Vector Index**  \n",
        "   - `VectorStoreIndex.from_documents(doc_objects, embed_model=embed_instance)` applies the specified embedding model (`embed_instance`) to each chunk of text. Essentially:\n",
        "     - For every chunk of your documents, the model creates a numerical vector (a list of floating-point numbers) that represents the semantic meaning of that text.  \n",
        "     - Those vectors are then stored in a specialized data structure (the “vector index”), which allows for efficient similarity search.  \n",
        "   - \\[**Why embed documents into vectors?**\\] Searching with embeddings (a “vector search” approach) is more powerful than keyword search alone, because it lets you find content that’s semantically similar even if it doesn’t match exact keywords.\n",
        "\n",
        "3. **Persisting the Index**  \n",
        "   - `doc_index.storage_context.persist('doc_index/')` saves this entire index (the vectors, metadata, structure, etc.) to disk in the `doc_index/` folder.  \n",
        "   - This step is important because generating vectors for all your documents can be computational and time intensive. By persisting (storing) it, you don’t need to re-run the embedding process every time you restart or re-run your notebook.  \n",
        "\n",
        "4. **Loading the Index**  \n",
        "   - Later (or on the next notebook session), you can do:\n",
        "     ```python\n",
        "     storage_ctx = StorageContext.from_defaults(persist_dir='doc_index/')\n",
        "     doc_index = load_index_from_storage(storage_ctx, embed_model=embed_instance)\n",
        "     ```\n",
        "     to **reload** the same index from disk.  \n",
        "   - This means your system immediately has the previously-created vector data available. You don’t have to re-embed all documents again, saving a lot of time and API calls.\n",
        "\n",
        "---\n",
        "\n",
        "### Why We Need This Step\n",
        "\n",
        "1. **Efficient, Semantic Search**: A vector index lets you retrieve relevant chunks by semantic similarity, rather than simple keyword matching.  \n",
        "\n",
        "2. **Chunking for Model Constraints**: Large documents might exceed the model’s token limit if processed in one go, so chunking is both a technical necessity (token limits) and a best practice (finer-grained search).  \n",
        "\n",
        "3. **Performance & Cost Optimization**: Persisting the index means you pay for embeddings only once. You won’t have to re-embed the same text every time you run the code—this can save significant time and money if you’re using a paid API.  \n",
        "\n",
        "In short, this block of code is the backbone of any retrieval-augmented system (RAG) : it turns raw text documents into a searchable vector representation and persists that representation so you can re-use it without repeating expensive operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAdKeRjKZrnn"
      },
      "source": [
        "## Step 5 – Configure the RAG Query Engine Tool\n",
        "\n",
        "• The persisted index is wrapped in a query engine that performs similarity searches over the documents.\n",
        "• This query engine is then encapsulated as a “tool” that the agent can use to quickly retrieve information from the static knowledge base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxeYgbwGZrnn"
      },
      "outputs": [],
      "source": [
        "search_engine = doc_index.as_query_engine(llm=model_instance, similarity_top_k=5)\n",
        "\n",
        "rag_tool_instance = QueryEngineTool.from_defaults(\n",
        "    search_engine,\n",
        "    name=\"paper_query_engine\",\n",
        "    description=\"Query engine using locally indexed research papers.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4q-hzNeZrnn"
      },
      "source": [
        "### Let's see what’s going on in the above cell\n",
        "\n",
        "### **1. Wrapping the Vector Index into a “Query Engine”**\n",
        "\n",
        "- **`doc_index.as_query_engine(llm=model_instance, similarity_top_k=5)`**  \n",
        "\n",
        "  - We already have a `doc_index` (the vector index). By calling `.as_query_engine()`, we transform it into a **RAG (Retrieval-Augmented Generation) style** query engine.  \n",
        "\n",
        "  That means, When you see `.as_query_engine()`, it transforms the vector index + LLM combination into a single object that:\n",
        "\n",
        "- Takes a question,  \n",
        "- Does an embedding-based similarity search against the indexed documents,  \n",
        "- Passes the retrieved results to your chosen LLM,  \n",
        "- Produces a combined, context-rich answer.\n",
        "\n",
        "  - **`llm=model_instance`**: You specify which language model to use for summarizing or synthesizing the retrieved chunks. Essentially, when a user query comes in:\n",
        "    1. The query is turned into an embedding and compared against the stored vectors in `doc_index`.  \n",
        "    2. The top `k` most similar chunks (`similarity_top_k=5`) are pulled up from the index.  \n",
        "    3. These retrieved chunks, along with the user’s question, are fed to the language model (`model_instance`) for a final answer or summary.  \n",
        "\n",
        "- **Why “query engine?”**  \n",
        "  - A query engine orchestrates the whole process of **1) embedding the user query**, **2) performing similarity search**, and **3) optionally letting the LLM summarize** or refine the retrieved chunks into a coherent response. This single object (the “query engine”) is easier to call than manually writing all these steps.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Creating a Tool for the Agent**\n",
        "\n",
        "- **`QueryEngineTool.from_defaults(search_engine, ...)`**  \n",
        "  - This line wraps the `search_engine` in a **Tool** that an agent (like a ReAct agent) can call programmatically. Tools are part of the “agent” ecosystem, letting your agent say, “I want to use this particular capability to retrieve knowledge.”  \n",
        "  - **`name=\"paper_query_engine\"`** and **`description=\"Query engine using locally indexed research papers.\"`** provide a label and a short explanation, so the agent knows what the tool does.  \n",
        "\n",
        "- **Why do we need a “Tool?”**  \n",
        "  - In many agent frameworks, the agent can choose from multiple specialized Tools (e.g. a “Calculator” tool, a “PDF Downloader” tool, a “Vector Query” tool).  \n",
        "  - The ReAct agent or any other sophisticated agent can look at your question, decide it needs to consult the local knowledge base, and “call” this tool with a query. The tool returns relevant chunks or a refined answer to the agent, which the agent then integrates into its final response.\n",
        "\n",
        "---\n",
        "\n",
        "### **Bottom Line**\n",
        "\n",
        "1. **RAG Query Engine**:  \n",
        "   The `.as_query_engine()` method transforms your vector index (already containing the embedded chunks of text) into an object that knows how to accept a natural language query, do similarity-based retrieval, and optionally refine the final answer using the language model.\n",
        "\n",
        "2. **Agent Tool**:  \n",
        "   Wrapping the query engine as a `QueryEngineTool` means you can embed that retrieval functionality into an agent that can call it like a function. The agent no longer needs to know *how* the search is done under the hood; it just knows there’s a “paper_query_engine” tool available if it wants to look up local research papers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ew80VNDZrnn"
      },
      "source": [
        "## Step 6 – Define the PDF Download Function\n",
        "\n",
        "• A simple function is created to download a PDF file from a provided URL and save it locally.\n",
        "• This functionality is critical when you want to keep a copy of a research paper for offline use or further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MosllhgJZrnn"
      },
      "outputs": [],
      "source": [
        "def download_pdf_file(pdf_url, destination):\n",
        "    try:\n",
        "        response = requests.get(pdf_url)\n",
        "        response.raise_for_status()\n",
        "        with open(destination, \"wb\") as out_file:\n",
        "            out_file.write(response.content)\n",
        "        return f\"PDF saved as '{destination}'.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dW97yWJZrnn"
      },
      "source": [
        "## Step 7 – Wrap Functions as Agent Tools\n",
        "\n",
        "• Both the ArXiv fetch function and the PDF download function are wrapped as tools.\n",
        "\n",
        "• This step lets the agent “call” these functions during a conversation.\n",
        "\n",
        "• The static RAG query engine is also wrapped as a tool, so the agent has 3 distinct abilities: search in the index, fetch new papers, and download PDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-ctS-u3Zrnn"
      },
      "outputs": [],
      "source": [
        "pdf_tool = FunctionTool.from_defaults(\n",
        "    download_pdf_file,\n",
        "    name=\"pdf_downloader\",\n",
        "    description=\"Downloads a PDF given a URL and saves it locally.\"\n",
        ")\n",
        "\n",
        "arxiv_tool = FunctionTool.from_defaults(\n",
        "    query_arxiv_papers,\n",
        "    name=\"arxiv_fetcher\",\n",
        "    description=\"Fetches recent research papers on a given topic from ArXiv.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rHK__D-Zrnn"
      },
      "source": [
        "## Step 8 – Integrating Everything with the ReAct Agent\n",
        "\n",
        "• Combine the 3 tools—RAG query engine, ArXiv paper fetcher, and PDF downloader—into a single agent using ReAct.\n",
        "\n",
        "• The agent first reasons about the query: it checks its knowledge base, and if no relevant papers are found, it will fetch from ArXiv dynamically.\n",
        "\n",
        "• The chat flow retains context so that a later command (like “Download the papers…”) refers to the previous result.\n",
        "\n",
        "• This orchestration is what allows the agent to combine a fixed knowledge base with dynamic ArXiv queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BX_VtJjZrnn"
      },
      "outputs": [],
      "source": [
        "react_agent = ReActAgent.from_tools(\n",
        "    [pdf_tool, rag_tool_instance, arxiv_tool],\n",
        "    llm=model_instance,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFVfYql0Zrno"
      },
      "source": [
        "## Step 9 – Interact with the Agent: Query for Papers\n",
        "\n",
        "• A formatted prompt is provided to ask for research papers on a topic (e.g., \"Brain-to-Text Decoding\").\n",
        "\n",
        "• The agent first checks its indexed documents using the RAG tool; if relevant papers are found, it returns their details.\n",
        "\n",
        "• If not, it will use the ArXiv fetch tool to get the latest papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHFykk6AZrno",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94722964-44bd-47d2-d020-e0e63944274c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Running step 9103e5a5-2a84-41be-89c1-1c842f01bbe6. Step input: I'm researching Brain-to-Text Decoding. \n",
            "Using your indexed database, please provide details such as title, abstract, authors, and a link for PDF download for papers related to Brain-to-Text Decoding. If nothing is available, fetch the latest ones from ArXiv.\n",
            "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
            "Action: paper_query_engine\n",
            "Action Input: {'input': 'Brain-to-Text Decoding'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Error: API error occurred: Status 429\n",
            "{\"message\":\"Requests rate limit exceeded\"}\n",
            "\u001b[0m> Running step 29ebbca2-6ead-4eb3-9433-f0a0eaf003ee. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I need to fetch the latest papers from ArXiv since the query engine is not available.\n",
            "Action: arxiv_fetcher\n",
            "Action Input: {'topic_str': 'Brain-to-Text Decoding', 'max_results': 5}\n",
            "\u001b[0m\u001b[1;3;34mObservation: []\n",
            "\u001b[0m> Running step acfc0fae-28c1-48f8-ab7e-cab566f40fcc. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
            "Answer: I'm sorry, but I couldn't find any recent research papers on \"Brain-to-Text Decoding\" from ArXiv.\n",
            "\u001b[0mI'm sorry, but I couldn't find any recent research papers on \"Brain-to-Text Decoding\" from ArXiv.\n"
          ]
        }
      ],
      "source": [
        "prompt_template = (\n",
        "    \"I'm researching {subject}. \\n\"\n",
        "    \"Using your indexed database, please provide details such as title, abstract, authors, and a link for PDF download for papers related to {subject}. \"\n",
        "    \"If nothing is available, fetch the latest ones from ArXiv.\"\n",
        ")\n",
        "\n",
        "response_one = react_agent.chat(prompt_template.format(subject=\"Brain-to-Text Decoding\"))\n",
        "\n",
        "print(response_one.response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "861Cx-T8Zrno"
      },
      "source": [
        "## Step 10 – Instruct the Agent to Download PDFs\n",
        "\n",
        "• A follow-up command tells the agent to download the papers it mentioned earlier.\n",
        "\n",
        "• This triggers the PDF download tool, which retrieves the PDFs using the provided links and saves them locally.\n",
        "\n",
        "------\n",
        "\n",
        "## So when exactly the agent goes for downloading a paper.\n",
        "\n",
        "In the overall project flow, the PDF download isn’t part of the initial query; it’s a follow-up action. Here’s how it works:\n",
        "\n",
        "• First, you send a query (using the prompt_template) asking for paper details on a topic (like \"Brain-to-Text Decoding\").  \n",
        " – The agent checks its local index and, if needed, fetches new papers from ArXiv, returning details (including PDF links).\n",
        "\n",
        "• After reviewing the returned paper details, you can issue a new command—for example, “Download the papers you mentioned” or “Save the PDFs for these papers.”  \n",
        " – At this point, the agent uses the conversation context (i.e., the papers it just talked about) and calls the PDF download tool to save the PDFs locally.\n",
        "\n",
        "So, you ask for the download after you receive the paper details and decide you want the actual PDF files saved. The project is designed so that the retrieval of paper details and the PDF downloading are separate steps, which gives you control over when you want to perform the download action.\n",
        "\n",
        "------\n",
        "\n",
        "### And ONLY the retrieval of paper details is handled by two components:\n",
        "\n",
        "• **RAG Query Engine Tool:**  \n",
        "\n",
        " - This tool searches your pre-indexed (stored) documents and returns details like title, abstract, authors, and PDF link. It’s used when the paper is already in your static knowledge base.\n",
        "\n",
        "• **ArXiv Fetch Function (`query_arxiv_papers`):**  \n",
        " - This function directly queries ArXiv to retrieve the latest paper details if they aren’t found in the index.\n",
        "\n",
        "Only after these retrieval steps do you later use the PDF download function (wrapped as a tool) when you explicitly instruct the agent to download the PDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Y84uYGwZrno",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11472fa0-9032-4a12-a025-e5dea1f4e8b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Running step f7b0a8e2-ed87-4cc4-b39b-54ef74673aa5. Step input: Download the papers you mentioned earlier.\n",
            "\u001b[1;3;34mObservation: Error: Could not parse output. Please follow the thought-action-input format. Try again.\n",
            "\u001b[0m> Running step 268acd6f-923a-4353-9e30-ad0018c80af7. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I need to use a tool to help me answer the question.\n",
            "Action: arxiv_fetcher\n",
            "Action Input: {'topic_str': 'Brain-to-Text Decoding', 'max_results': 3}\n",
            "\u001b[0m\u001b[1;3;34mObservation: []\n",
            "\u001b[0m> Running step 2a8f25d7-8a49-4c0d-9182-ba9a3314e4f8. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I cannot find any papers related to Brain-to-Text Decoding in the indexed database or on ArXiv.\n",
            "Answer: I'm sorry, but I couldn't find any recent research papers on \"Brain-to-Text Decoding\" from ArXiv.\n",
            "\u001b[0mI'm sorry, but I couldn't find any recent research papers on \"Brain-to-Text Decoding\" from ArXiv.\n"
          ]
        }
      ],
      "source": [
        "response_two = react_agent.chat(\"Download the papers you mentioned earlier.\")\n",
        "\n",
        "print(response_two.response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuiGYMq-Zrno"
      },
      "source": [
        "## Step 11 – Test Dynamic ArXiv Fetch for Unindexed Topics\n",
        "\n",
        "• Finally, you test the agent with a topic that isn’t in the static index (e.g., “Gaussian process”).\n",
        "\n",
        "• The agent uses the ArXiv fetch tool to dynamically retrieve new papers on this topic.\n",
        "\n",
        "• This shows the system’s flexibility—combining static retrieval with on-demand querying.\n",
        "\n",
        "Finally, query the agent with a topic that isn’t in the index (e.g., “Gaussian process”) to see the ArXiv fetching in action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wc1ziID2Zrno",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55f22366-0814-4da2-e264-fecbc2d356bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Running step c7f7143d-dbf0-43b5-bd07-1553de72845a. Step input: I'm researching Gaussian process. \n",
            "Using your indexed database, please provide details such as title, abstract, authors, and a link for PDF download for papers related to Gaussian process. If nothing is available, fetch the latest ones from ArXiv.\n",
            "\u001b[1;3;38;5;200mThought: I need to use a tool to help me answer the question.\n",
            "Action: paper_query_engine\n",
            "Action Input: {'input': 'Gaussian process'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Error: API error occurred: Status 429\n",
            "{\"message\":\"Requests rate limit exceeded\"}\n",
            "\u001b[0m> Running step 5bac101b-f9af-44b8-800d-da0ef5466c87. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I need to fetch the latest papers from ArXiv.\n",
            "Action: arxiv_fetcher\n",
            "Action Input: {'topic_str': 'Gaussian process', 'max_results': 3}\n",
            "\u001b[0m\u001b[1;3;34mObservation: [{'paper_title': 'Kernel dependence of the Gaussian Process reconstruction of late Universe expansion history', 'authors_list': ['Joseph P Johnson', 'H. K. Jassal'], 'abstract_text': 'In this work, we discuss model-independent reconstruction of the expansion\\nhistory of the late Universe. We use Gaussian Process Regression to reconstruct\\nthe evolution of various cosmological parameters such as H(z) and slow-roll\\nparameter using observational data to train the GP model. We look at the GP\\nreconstruction of these parameters using stationary and non-stationary kernel\\nfunctions. We examine the effect of the choice of kernel functions on the\\nreconstructions. We find that non-stationary kernels such as polynomial kernels\\nmight be a better choice for the reconstruction if the training data set is\\nnoisy (such as H(z) data) as it helps to avoid fitting the error in the data.\\nWe also look at the kernel dependence of other cosmological parameters such as\\nthe redshift of transition to the accelerated expansion. This has been achieved\\nby reconstructing the derivatives of the expansion history (H(z)) such as the\\ndeceleration parameter/slow-roll parameter.', 'date_published': datetime.datetime(2025, 3, 6, 10, 3, 8, tzinfo=datetime.timezone.utc), 'journal_details': None, 'doi_number': None, 'primary_category': 'astro-ph.CO', 'all_categories': ['astro-ph.CO', 'gr-qc'], 'pdf_link': 'http://arxiv.org/pdf/2503.04273v1', 'arxiv_link': 'http://arxiv.org/abs/2503.04273v1'}, {'paper_title': 'Mixed Likelihood Variational Gaussian Processes', 'authors_list': ['Kaiwen Wu', 'Craig Sanders', 'Benjamin Letham', 'Phillip Guan'], 'abstract_text': 'Gaussian processes (GPs) are powerful models for human-in-the-loop\\nexperiments due to their flexibility and well-calibrated uncertainty. However,\\nGPs modeling human responses typically ignore auxiliary information, including\\na priori domain expertise and non-task performance information like user\\nconfidence ratings. We propose mixed likelihood variational GPs to leverage\\nauxiliary information, which combine multiple likelihoods in a single evidence\\nlower bound to model multiple types of data. We demonstrate the benefits of\\nmixing likelihoods in three real-world experiments with human participants.\\nFirst, we use mixed likelihood training to impose prior knowledge constraints\\nin GP classifiers, which accelerates active learning in a visual perception\\ntask where users are asked to identify geometric errors resulting from camera\\nposition errors in virtual reality. Second, we show that leveraging Likert\\nscale confidence ratings by mixed likelihood training improves model fitting\\nfor haptic perception of surface roughness. Lastly, we show that Likert scale\\nconfidence ratings improve human preference learning in robot gait\\noptimization. The modeling performance improvements found using our framework\\nacross this diverse set of applications illustrates the benefits of\\nincorporating auxiliary information into active learning and preference\\nlearning by using mixed likelihoods to jointly model multiple inputs.', 'date_published': datetime.datetime(2025, 3, 6, 6, 33, 58, tzinfo=datetime.timezone.utc), 'journal_details': None, 'doi_number': None, 'primary_category': 'cs.LG', 'all_categories': ['cs.LG', 'stat.ML'], 'pdf_link': 'http://arxiv.org/pdf/2503.04138v1', 'arxiv_link': 'http://arxiv.org/abs/2503.04138v1'}, {'paper_title': 'Real-time Spatial-temporal Traversability Assessment via Feature-based Sparse Gaussian Process', 'authors_list': ['Senming Tan', 'Zhenyu Hou', 'Zhihao Zhang', 'Long Xu', 'Mengke Zhang', 'Zhaoqi He', 'Chao Xu', 'Fei Gao', 'Yanjun Cao'], 'abstract_text': 'Terrain analysis is critical for the practical application of ground mobile\\nrobots in real-world tasks, especially in outdoor unstructured environments. In\\nthis paper, we propose a novel spatial-temporal traversability assessment\\nmethod, which aims to enable autonomous robots to effectively navigate through\\ncomplex terrains. Our approach utilizes sparse Gaussian processes (SGP) to\\nextract geometric features (curvature, gradient, elevation, etc.) directly from\\npoint cloud scans. These features are then used to construct a high-resolution\\nlocal traversability map. Then, we design a spatial-temporal Bayesian Gaussian\\nkernel (BGK) inference method to dynamically evaluate traversability scores,\\nintegrating historical and real-time data while considering factors such as\\nslope, flatness, gradient, and uncertainty metrics. GPU acceleration is applied\\nin the feature extraction step, and the system achieves real-time performance.\\nExtensive simulation experiments across diverse terrain scenarios demonstrate\\nthat our method outperforms SOTA approaches in both accuracy and computational\\nefficiency. Additionally, we develop an autonomous navigation framework\\nintegrated with the traversability map and validate it with a differential\\ndriven vehicle in complex outdoor environments. Our code will be open-source\\nfor further research and development by the community,\\nhttps://github.com/ZJU-FAST-Lab/FSGP_BGK.', 'date_published': datetime.datetime(2025, 3, 6, 6, 26, 57, tzinfo=datetime.timezone.utc), 'journal_details': None, 'doi_number': None, 'primary_category': 'cs.RO', 'all_categories': ['cs.RO', 'cs.CV'], 'pdf_link': 'http://arxiv.org/pdf/2503.04134v1', 'arxiv_link': 'http://arxiv.org/abs/2503.04134v1'}]\n",
            "\u001b[0m> Running step 8d211d32-cca9-414e-85b4-a9ae2f23d2e8. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
            "Answer: Here are the details of the papers related to Gaussian process:\n",
            "\n",
            "1. **Title:** Kernel dependence of the Gaussian Process reconstruction of late Universe expansion history\n",
            "   - **Authors:** Joseph P Johnson, H. K. Jassal\n",
            "   - **Abstract:** In this work, we discuss model-independent reconstruction of the expansion history of the late Universe. We use Gaussian Process Regression to reconstruct the evolution of various cosmological parameters such as H(z) and slow-roll parameter using observational data to train the GP model. We look at the GP reconstruction of these parameters using stationary and non-stationary kernel functions. We examine the effect of the choice of kernel functions on the reconstructions. We find that non-stationary kernels such as polynomial kernels might be a better choice for the reconstruction if the training data set is noisy (such as H(z) data) as it helps to avoid fitting the error in the data. We also look at the kernel dependence of other cosmological parameters such as the redshift of transition to the accelerated expansion. This has been achieved by reconstructing the derivatives of the expansion history (H(z)) such as the deceleration parameter/slow-roll parameter.\n",
            "   - **PDF Link:** [Download PDF](http://arxiv.org/pdf/2503.04273v1)\n",
            "\n",
            "2. **Title:** Mixed Likelihood Variational Gaussian Processes\n",
            "   - **Authors:** Kaiwen Wu, Craig Sanders, Benjamin Letham, Phillip Guan\n",
            "   - **Abstract:** Gaussian processes (GPs) are powerful models for human-in-the-loop experiments due to their flexibility and well-calibrated uncertainty. However, GPs modeling human responses typically ignore auxiliary information, including a priori domain expertise and non-task performance information like user confidence ratings. We propose mixed likelihood variational GPs to leverage auxiliary information, which combine multiple likelihoods in a single evidence lower bound to model multiple types of data. We demonstrate the benefits of mixing likelihoods in three real-world experiments with human participants. First, we use mixed likelihood training to impose prior knowledge constraints in GP classifiers, which accelerates active learning in a visual perception task where users are asked\n",
            "\u001b[0mHere are the details of the papers related to Gaussian process:\n",
            "\n",
            "1. **Title:** Kernel dependence of the Gaussian Process reconstruction of late Universe expansion history\n",
            "   - **Authors:** Joseph P Johnson, H. K. Jassal\n",
            "   - **Abstract:** In this work, we discuss model-independent reconstruction of the expansion history of the late Universe. We use Gaussian Process Regression to reconstruct the evolution of various cosmological parameters such as H(z) and slow-roll parameter using observational data to train the GP model. We look at the GP reconstruction of these parameters using stationary and non-stationary kernel functions. We examine the effect of the choice of kernel functions on the reconstructions. We find that non-stationary kernels such as polynomial kernels might be a better choice for the reconstruction if the training data set is noisy (such as H(z) data) as it helps to avoid fitting the error in the data. We also look at the kernel dependence of other cosmological parameters such as the redshift of transition to the accelerated expansion. This has been achieved by reconstructing the derivatives of the expansion history (H(z)) such as the deceleration parameter/slow-roll parameter.\n",
            "   - **PDF Link:** [Download PDF](http://arxiv.org/pdf/2503.04273v1)\n",
            "\n",
            "2. **Title:** Mixed Likelihood Variational Gaussian Processes\n",
            "   - **Authors:** Kaiwen Wu, Craig Sanders, Benjamin Letham, Phillip Guan\n",
            "   - **Abstract:** Gaussian processes (GPs) are powerful models for human-in-the-loop experiments due to their flexibility and well-calibrated uncertainty. However, GPs modeling human responses typically ignore auxiliary information, including a priori domain expertise and non-task performance information like user confidence ratings. We propose mixed likelihood variational GPs to leverage auxiliary information, which combine multiple likelihoods in a single evidence lower bound to model multiple types of data. We demonstrate the benefits of mixing likelihoods in three real-world experiments with human participants. First, we use mixed likelihood training to impose prior knowledge constraints in GP classifiers, which accelerates active learning in a visual perception task where users are asked\n"
          ]
        }
      ],
      "source": [
        "response_three = react_agent.chat(prompt_template.format(subject=\"Gaussian process\"))\n",
        "\n",
        "print(response_three.response)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}